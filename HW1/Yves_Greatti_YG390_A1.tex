\documentclass[11pt]{article}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[psamsfonts]{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}

\def\Kset{\mathbb{K}}
\def\Nset{\mathbb{N}}
\def\Qset{\mathbb{Q}}
\def\Rset{\mathbb{R}}
\def\Sset{\mathbb{S}}
\def\Zset{\mathbb{Z}}
\def\squareforqed{\hbox{\rlap{$\sqcap$}$\sqcup$}}
\def\qed{\ifmmode\squareforqed\else{\unskip\nobreak\hfil
\penalty50\hskip1em\null\nobreak\hfil\squareforqed
\parfillskip=0pt\finalhyphendemerits=0\endgraf}\fi}

\DeclareMathOperator*{\E}{\rm E}
\DeclareMathOperator*{\argmax}{\rm argmax}
\DeclareMathOperator*{\argmin}{\rm argmin}
\DeclareMathOperator{\sgn}{sign}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\last}{last}
\DeclareMathOperator{\sign}{\sgn}
\DeclareMathOperator{\diag}{diag}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\def\vcdim{\textnormal{VCdim}}

\newcommand{\cX}{{\mathcal X}}
\newcommand{\cY}{{\mathcal Y}}
\newcommand{\cA}{{\mathcal A}}
\newcommand{\ignore}[1]{}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bd}{\begin{description}}
\newcommand{\ed}{\end{description}}
\newcommand{\h}{\widehat}
\newcommand{\e}{\epsilon}
\newcommand{\mat}[1]{{\mathbf #1}}
\newcommand{\R}{\mat{R}}
\newcommand{\0}{\mat{0}}
\newcommand{\M}{\mat{M}}
\newcommand{\D}{\mat{D}}
\renewcommand{\r}{\mat{r}}
\newcommand{\x}{\mat{x}}
\renewcommand{\u}{\mat{u}}
\renewcommand{\v}{\mat{v}}
\newcommand{\w}{\mat{w}}
\renewcommand{\H}{\text{0}}
\newcommand{\T}{\text{1}}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\xxi}{{\boldsymbol \xi}}
\newcommand{\ssigma}{{\boldsymbol \sigma}}
\newcommand{\Alpha}{{\boldsymbol \alpha}}
\newcommand{\tts}{\tt \small}
\newcommand{\hint}{\emph{hint}}
\newcommand{\matr}[1]{\bm{#1}}     % ISO complying version
\newcommand{\vect}[1]{\bm{#1}}     % ISO complying version

\newenvironment{solution}{\vspace{.25cm}\noindent{\it Solution:}}{}

\begin{document}

\noindent DS-GA 1008:\\
Deep Learning, Spring 2019\\
Homework Assignment 1 \\
Yves Greatti - yg390\\

\section{Backpropagation}

Backpropagation or “backward propagation through errors” is a method which calculates the gradient of the loss function of a neural network with respect to its weights.

\subsection{Warm-up}
The chain rule is at the heart of backpropagation. Assume you are given input  $\vect{x}$ and output $\vect{y}$, both in $\Rset^2$, and the error backpropagated to the output is $\frac{ \partial L}{\partial y}$. In particular, let

\begin{equation*}
\vect{y} = \matr{W} \vect{x} + \vect{b}
\end{equation*}

where $\matr{W} \in \Rset^{2x2}$ and $\vect{x}$, $\vect{b} \in \Rset^2$. Give an expression for   $\frac{ \partial L}{\partial \matr{W}}$ and  $\frac{ \partial L}{\partial \vect{b}}$ in terns of $\frac{ \partial L}{\partial \vect{y}}$ and $\vect{x}$ using the chain rule.

\begin{align*}
	\frac{ \partial L}{\partial \matr{W}} &= \frac{ \partial \vect{y}}{\partial \matr{W}} \cdot \frac{ \partial L}{\partial \vect{y}} \\
	&= \sum_{i=1}^2 \frac{ \partial y_i}{\partial \matr{W}} \Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_i \\
	&=  \Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_1 
		\begin{bmatrix}
   			\rule[0.5mm]{0.8cm}{0.1mm} \; \vect{x}^{\top} \; \rule[0.5mm]{0.8cm}{0.1mm} \\
   			\rule[0.5mm]{0.8cm}{0.1mm} \; \vect{0}^{\top} \; \rule[0.5mm]{0.8cm}{0.1mm} \\
		\end{bmatrix} + 
		\Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_2
		\begin{bmatrix}
   			\rule[0.5mm]{0.8cm}{0.1mm} \; \vect{0}^{\top} \; \rule[0.5mm]{0.8cm}{0.1mm} \\
   			\rule[0.5mm]{0.8cm}{0.1mm} \; \vect{x}^{\top} \; \rule[0.5mm]{0.8cm}{0.1mm} \\
		\end{bmatrix} \\
	&=  \begin{bmatrix}
		\big(\frac{ \partial L}{\partial \vect{y}}\big)_1  \cdot \vect{x}^{\top} \\
		\big(\frac{ \partial L}{\partial \vect{y}}\big)_2  \cdot \vect{x}^{\top}  \\
	      \end{bmatrix} \\
	&= \frac{ \partial L}{\partial \vect{y}} \vect{x}^{\top}
\end{align*}
We can write this as an outer product:
\begin{equation*}
    \frac{ \partial L}{\partial \matr{W}}^\top = \frac{ \partial L}{\partial \vect{y}}^\top \otimes \vect{x}^{\top}
\end{equation*}

Now we have for  $\frac{ \partial L}{\partial \vect{b}}$ :
\begin{align*}
	\frac{ \partial L}{\partial \vect{b}} &= \frac{ \partial \vect{y}} {\partial \vect{b}} \cdot \frac{ \partial L}{\partial \vect{y}}  \\
	&= \sum_{i=1}^2 \frac{ \partial y_i}{\partial \vect{b}}  \Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_i \\
	&= 1 \cdot \Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_1 + 1 \cdot \Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_2 \\
	&=  \sum_{i=1}^2  \Bigg( \frac{ \partial L}{\partial \vect{y}} \Bigg)_i
\end{align*}

\subsection{Softmax}
Multinomial logistic regression is a generalization of logistic regression into multiple classes. The softmax expression is at the crux of this technique. After receiving n unconstrained values, the softmax function normalizes these values to n values that all sum to 1. This can then be perceived as probabilities attributed to the various classes by a classifier. Your task here is to back-propagate error through this module. The softmax expression which indicates the probability of the j-th class is as follows:
\[
	\mathrm {P}(z = j \; | \; \vect{x} ) = y_j = \frac{\exp(\beta x_j)}{\sum_i \exp(\beta x_i)}
\]

What is the expression for  $\frac{\partial y_j}{x_i}$? (Hint: Answer differs when i = j and i $\ne$ j)
Note that the variables $\vect{x}$ and $\vect{y}$ aren’t scalars but vectors. 
While $\vect{x}$ represents the n values input to the system,  $\vect{y}$ represents the n probabilities output from the system. Therefore, the expression $y_j$ represents the j-th element of $\vect{y}$.

\begin{solution}
From the quotient rule for differential functions, we have:
\[
	\frac{\partial y_j}{\partial x_i} = \frac{\sum_k \exp(\beta x_k) \cdot \frac{\partial e^{\beta x_j}}{\partial x_i}  - e^{\beta x_j} \frac{\partial \sum_k \exp(\beta x_k)} {\partial x_i}} {\bigg(\sum_k \exp(\beta x_k)\bigg)^2}
\]

if $i=j$ then:
\begin{align*}
	\frac{\partial e^{\beta x_j}}{\partial x_i}  &= \beta  e^{\beta x_i} \\
	 \frac{\partial \sum_k \exp(\beta x_k)} {\partial x_i} &= \beta e^{\beta x_i}
\end{align*}
Thus

\begin{align*}
	\frac{\partial y_i}{\partial x_i} &=  \frac{\sum_k \exp(\beta x_k) \cdot \beta  e^{\beta x_i} -  \beta e^{\beta x_i}  e^{\beta x_i} }  {\bigg(\sum_k \exp(\beta x_k)\bigg)^2} \\
	&= \beta \frac{\exp(\beta x_i)} {\sum_k \exp(\beta x_k)} \Bigg(1- \frac{\exp(\beta x_i)}{{\sum_k \exp(\beta x_k)}} \Bigg)
\end{align*}

if $i \ne j$: $\frac{\partial e^{\beta x_j}}{\partial x_i} = 0$ then:

\[
	\frac{\partial y_j}{\partial x_i} = -  \frac{\beta e^{\beta x_i}  e^{\beta x_j} }  {\bigg(\sum_k \exp(\beta x_k)\bigg)^2} 
\]

So the backpropagation of the error through softmax is:
\begin{equation}
\frac{\partial y_j}{\partial x_i}=
\begin{cases}
 \beta  \; p_i  \; (1-p_i) ~~\text{if } i = j\\    
 -\beta \;  p_i \; p_j ~~ \text{otherwise}
\end{cases}
\text{where } p_j = y_j = \frac{\exp(\beta x_j)}{\sum_i \exp(\beta x_i)} 
\end{equation}
 
 Or using Kronecker delta $\delta_{ij}$:
 \begin{equation}
	\frac{\partial y_j}{\partial x_i} =  \beta  \; p_i  \; (\delta_{ij}-p_j) 
\end{equation}

 \end{solution}

\end{document}
