{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from plot_lib import set_default, show_scatterplot, plot_bases\n",
    "from matplotlib.pyplot import plot, title, axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "seed = 1008\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Full, slice, fill\n",
    "Write a function `warm_up` that returns the 2D tensor with integers below. **Do not use any loops**.\n",
    "\n",
    "```\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "```\n",
    "\n",
    "\n",
    "Hint: Use `torch.full`, `torch.fill_`, and the slicing operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
      "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
      "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
      "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
      "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
      "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
      "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "def warm_up():\n",
    "    t = torch.IntTensor(13, 13)\n",
    "    t.fill_(2)\n",
    "    r = torch.IntTensor(13)\n",
    "    r.fill_(2)\n",
    "    r[0] = 1\n",
    "    r[-1] = 1\n",
    "    r[-2] = 2\n",
    "    r1 = torch.full((4,), 1, dtype=torch.int32)\n",
    "    r[2:6] = r1\n",
    "    r[7:11] = r1\n",
    "    t33 = torch.full((2,2), 3, dtype=torch.int32)\n",
    "    t11 = torch.IntTensor(4,4)\n",
    "    t11.fill_(1)\n",
    "    t11[1:3, 1:3] = t33\n",
    "    t22 = torch.IntTensor(6,6)\n",
    "    t22.fill_(2)\n",
    "    t22[1:5, 1:5] = t11\n",
    "    t[0] = r\n",
    "    t[-1] = r\n",
    "    rt = r.view(r.shape[0], 1).view(1, r.shape[0])\n",
    "    t[:,0] = rt\n",
    "    t[:,-1] = rt\n",
    "    t[1:7,1:7] = t22\n",
    "    t[1:7,6:12] = t22\n",
    "    t[6:12,1:7] = t22\n",
    "    t[6:12,6:12] = t22\n",
    "    return t\n",
    "\n",
    "\n",
    "# Uncomment line below once you implement this function. \n",
    "print(warm_up())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. To Loop or not to loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. `mul_row_loop`\n",
    "Write a function `mul_row_loop`, using python loops (and not even slicing operators), that gets a 2D tensor\n",
    "as input, and returns a tensor of same size, equal to the one given as argument, with the first row\n",
    "kept unchanged, the second multiplied by two, the third by three, etc.\n",
    "For instance:\n",
    "```\n",
    ">>> t = torch.full((4, 8), 2.0)\n",
    ">>> t\n",
    "tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[2., 2., 2., 2., 2., 2., 2., 2.]])\n",
    ">>> mul_row(t)\n",
    "tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[4., 4., 4., 4., 4., 4., 4., 4.],\n",
    "[6., 6., 6., 6., 6., 6., 6., 6.],\n",
    "[8., 8., 8., 8., 8., 8., 8., 8.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [6., 6., 6., 6., 6., 6., 6., 6.],\n",
      "        [8., 8., 8., 8., 8., 8., 8., 8.]])\n"
     ]
    }
   ],
   "source": [
    "def mul_row_loop(input_tensor):\n",
    "    output_tensor = torch.Tensor(input_tensor.shape)\n",
    "    n = input_tensor.shape[0]\n",
    "    for i in range(n):\n",
    "        output_tensor[i] = input_tensor[i] * (i+1)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. `mul_row_fast`\n",
    "Write a second version of the same function named `mul_row_fast` which uses tensor operations and no looping.\n",
    "\n",
    "**Hint**: Use broadcasting and `torch.arange`, `torch.view`, and `torch.mul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4., 6., 8., 2., 4., 6., 8.],\n",
      "        [2., 4., 6., 8., 2., 4., 6., 8.],\n",
      "        [2., 4., 6., 8., 2., 4., 6., 8.],\n",
      "        [2., 4., 6., 8., 2., 4., 6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "def mul_row_fast(input_tensor):\n",
    "    r  = torch.mul(input_tensor.view(input_tensor.shape[1], input_tensor.shape[0]), \n",
    "                   torch.arange(1,input_tensor.shape[0]+1, dtype=torch.float32).view(1, _.shape[0]))\n",
    "    return r.view(r.shape[1], r.shape[0])\n",
    "\n",
    "def mul_row_fast_2(input_tensor):\n",
    "    t1 = input_tensor.view(input_tensor.shape[1], input_tensor.shape[0])\n",
    "    t2 = torch.arange(1,input_tensor.shape[0]+1, dtype=torch.float32)\n",
    "    t2 = t2.view(1, t2.shape[0])    \n",
    "    r  = torch.mul(t1,t2)\n",
    "    r = r.view(r.shape[1], r.shape[0])\n",
    "    return r\n",
    "\n",
    "t = torch.full((4, 8), 2.0)\n",
    "print(mul_row_fast(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. `times`\n",
    "Write a function `times` which takes a 2D tensor as input and returns the run times of `mul_row_loop` and `mul_row_fast` on this tensor, respectively. Use `time.perf_counter`.\n",
    "\n",
    "Use `torch.ones` to create a 2D tensor of size (1000, 400) full of ones and run `times` on it (there should be more than two orders of magnitude difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018058394081890583, 0.00025727110914886\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "def times(input_tensor):\n",
    "    t_0 = perf_counter()\n",
    "    mul_row_loop(input_tensor)\n",
    "    time_1 = perf_counter() - t_0\n",
    "    t_0 = perf_counter()\n",
    "    mul_row_fast_2(input_tensor)\n",
    "    time_2 = perf_counter() - t_0\n",
    "    return time_1, time_2\n",
    "\n",
    "# Uncomment lines below once you implement this function. \n",
    "random_tensor = torch.ones(1000, 400)\n",
    "time_1, time_2 = times(random_tensor)\n",
    "print('{}, {}'.format(time_1, time_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. ReLU\n",
    "ReLU (Rectified Linear Unit) is a non-linear activation fuction defined as:\n",
    "\n",
    "$$y = \\mathrm{max}(0,x)$$\n",
    "\n",
    "Define a fully connected neural network `linear_fc_relu` which:\n",
    "- takes 2 dimensional data as input and passes it through linear modules (`torch.nn.Linear`)\n",
    "- has one hidden layer of dimension 5 \n",
    "- has output dimension of 2\n",
    "- has ReLu as an activation function\n",
    "\n",
    "Create a tensor with input data $X$ of size (100, 2) using `torch.randn`. \n",
    "\n",
    "Following the example in https://github.com/Atcold/pytorch-Deep-Learning-Minicourse/blob/master/02-space_stretching.ipynb, visualize the output of passing `X` through the neural network `linear_fc_relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "X = torch.randn(100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 1-layer neural networks with ReLU activation\n",
    "linear_fc_relu = nn.Sequential(\n",
    "            nn.Linear(2, n_hidden), \n",
    "            NL, \n",
    "            nn.Linear(n_hidden, 2)\n",
    "        )\n",
    "# Visualize: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scatterplot(X, colors, title='x')\n",
    "n_hidden = 5\n",
    "\n",
    "NL = nn.ReLU()\n",
    "#NL = nn.Tanh()\n",
    "\n",
    "for i in range(5):\n",
    "    # create 1-layer neural networks with random weights\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(2, n_hidden), \n",
    "            NL, \n",
    "            nn.Linear(n_hidden, 2)\n",
    "        )\n",
    "    with torch.no_grad():\n",
    "        Y = model(X)\n",
    "    show_scatterplot(Y, colors, title='f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Sigmoid\n",
    "The sigmoid function is another popular choice for a non-linear activation function which maps its input to values in the interval $(0,1)$. It is formally defined as:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+ e^{-x}}$$\n",
    "\n",
    "Define a new neural network `linear_fc_sigmoid` which is the same architecture as in part `3.1.` but with a sigmoid unit instead of ReLU. \n",
    "\n",
    "Using the same $X$ as in part `3.1`, visualize the output of passing `X` through the neural network `linear_fc_sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 1-layer neural networks with Sigmoid activation\n",
    "# linear_fc_sigmoid = TODO\n",
    "# Visualize: TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
