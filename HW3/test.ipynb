{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "seed = 1008\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CrossEntropyLoss:  tensor(1.8024)\n",
      "Torch NLL loss:  tensor(1.8024)\n",
      "tensor([[-2.3975, -1.8599, -0.8858, -1.5456, -2.0572],\n",
      "        [-4.1107, -1.3640, -1.1521, -2.0161, -1.2772],\n",
      "        [-1.2443, -1.4025, -1.6459, -2.6446, -1.5994]])\n",
      "tensor([-2.3975, -1.3640, -1.6459])\n",
      "Custom NLL loss:  tensor(1.8024)\n"
     ]
    }
   ],
   "source": [
    "def NLLLoss(logs, targets):\n",
    "    out = torch.zeros_like(targets, dtype=torch.float)\n",
    "    print(logs)\n",
    "    for i in range(len(targets)):\n",
    "        out[i] = logs[i][targets[i]]\n",
    "    print(out)\n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "x = torch.randn(3, 5)\n",
    "y = torch.LongTensor([0, 1, 2])\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "x_log = log_softmax(x)\n",
    "\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "print(\"Torch CrossEntropyLoss: \", cross_entropy_loss(x, y))\n",
    "print(\"Torch NLL loss: \", nll_loss(x_log, y))\n",
    "print(\"Custom NLL loss: \", NLLLoss(x_log, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the MNIST training set with batch size 128, apply data shuffling and normalization\n",
    "batch_size=128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the MNIST test set with batch size 128, apply data shuffling and normalization\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval = 100):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Loop through data points\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        # Send data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero out the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass data through model\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute the negative log likelihood loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Make a step with the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss (uncomment lines below once implemented)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define test method\n",
    "def test(model, device, test_loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop through data points\n",
    "        for data, target in test_loader:\n",
    "        \n",
    "            # Send data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        \n",
    "            # Pass data through model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "            \n",
    "            # Get predictions from the model for each data point\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "            \n",
    "            # Add number of correct predictions to total num_correct \n",
    "            num_correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    # Compute the average test_loss\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=(5, 5))\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=(5, 5))\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=(5, 5))\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'1.{x.size()}')\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(f'2.{x.size()}')\n",
    "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=2)\n",
    "        print(f'3.{x.size()}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(f'4.{x.size()}')\n",
    "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(f'5.{x.size()}')\n",
    "        x = x.view(-1, 120) # reshaping\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(f'6.{x.size()}')\n",
    "        x = self.fc2(x)\n",
    "        print(f'7.{x.size()}')\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 20])\n",
      "torch.Size([600])\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(3, 10, 20)\n",
    "print(image.size())\n",
    "print(image.view(-1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
